{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "2000 documents\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading dataset\")\n",
    "\n",
    "from glob import glob\n",
    "filenames_neg = sorted(glob(op.join('..', 'data', 'imdb1', 'neg', '*.txt')))\n",
    "filenames_pos = sorted(glob(op.join('..', 'data', 'imdb1', 'pos', '*.txt')))\n",
    "\n",
    "texts_neg = [open(f).read() for f in filenames_neg]\n",
    "texts_pos = [open(f).read() for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print(\"%d documents\" % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(texts):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "        n_samples == number of documents.\n",
    "        n_features == number of words in vocabulary.\n",
    "    \"\"\"\n",
    "    vocabulary = Counter([])\n",
    "    for text in texts:\n",
    "        #pass\n",
    "        list1 = text.split()\n",
    "        dir1 = Counter(list1)\n",
    "        vocabulary = vocabulary + dir1\n",
    "    vocabulary = dict(vocabulary)\n",
    "    n_features = len(vocabulary)\n",
    "\n",
    "    counts = np.zeros(n_features)\n",
    "    j=0\n",
    "    for i in vocabulary:\n",
    "        counts[j] = vocabulary[i]\n",
    "        j +=1\n",
    "    \n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CountDocsInClass(y,c):\n",
    "    count = 0\n",
    "    for i in y:\n",
    "        if i == c:\n",
    "            count = count + 1\n",
    "    return count\n",
    "\n",
    "def ConcatenateTextOfAllDocsInClass(D1, c):\n",
    "    y = zip(*D1)[1]\n",
    "    D = zip(*D1)[0]\n",
    "    text_c = ''\n",
    "    for i in np.arange(0, len(D)):\n",
    "        if y[i] == c:\n",
    "            text_c = text_c + D[i]\n",
    "    return text_c\n",
    "\n",
    "def ExtractTokensFromDoc(V, d):\n",
    "    d = d.split()\n",
    "    i = 0\n",
    "    W = []\n",
    "    for v in V:\n",
    "        if v in d:\n",
    "            W.append(i)\n",
    "        i +=1\n",
    "    return W  \n",
    "\n",
    "def TrainMultinomialNB(C, D1):  \n",
    "    y = zip(*D1)[1]\n",
    "    D = zip(*D1)[0]\n",
    "    V,counts = count_words(D)\n",
    "    N = len(D)\n",
    "    prior = np.zeros(len(C))\n",
    "    condprob = np.zeros((len(V), len(C)))\n",
    "    i = 0\n",
    "    for c in C:\n",
    "        Nc = CountDocsInClass(y,c)\n",
    "        prior[i] = float(Nc)/N \n",
    "        text_c = ConcatenateTextOfAllDocsInClass(D1, c)\n",
    "        \n",
    "        V1 = text_c.split()\n",
    "        T_ct = np.zeros(len(V))\n",
    "        ii = 0\n",
    "        for m in V:\n",
    "            for l in V1:\n",
    "                if l == m:\n",
    "                    T_ct[ii] += 1\n",
    "            ii += 1\n",
    "\n",
    "        j = 0\n",
    "        for t in V:\n",
    "            condprob[j][i] = float(T_ct[j]+1)/(sum(T_ct+1)) \n",
    "            j += 1\n",
    "        i += 1\n",
    "    return V, prior, condprob\n",
    "\n",
    "def ApplyMultinomialNB(C,V, prior, condprob, d):\n",
    "    W = ExtractTokensFromDoc(V, d)\n",
    "    score = np.zeros(len(C))\n",
    "    i = 0\n",
    "    for c in C:\n",
    "        score[i] = np.log(prior[i])\n",
    "        for j in W:\n",
    "            score[i] += np.log(condprob[j][i])\n",
    "        i = i + 1\n",
    "    if score[0] > score[1] :\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        D1 = zip(X, y)\n",
    "        self.C = [0, 1]\n",
    "        self.V, self.prior, self.condprob = TrainMultinomialNB(self.C, D1)\n",
    "        return\n",
    "\n",
    "    def predict(self, X):\n",
    "        d = X\n",
    "        return ApplyMultinomialNB(self.C,self.V, self.prior, self.condprob, d)\n",
    "        #return (np.random.randn(len(X)) > 0).astype(np.int)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        correct = 0\n",
    "        for i in np.arange(0, len(X)):\n",
    "            if ApplyMultinomialNB(self.C,self.V, self.prior, self.condprob, X[i]) == y[i]:\n",
    "                correct += 1\n",
    "        return float(correct) / len(X)  \n",
    "        #return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NB()\n",
    "nb.fit(texts[::2], y[::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score is 0.836\n"
     ]
    }
   ],
   "source": [
    "print \"The score is \" + str(nb.score(texts[1::2], y[1::2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation \n",
    "X_cv = texts[400:525] + texts[1400:1525]\n",
    "y_cv = np.zeros(250)\n",
    "y_cv[:125] = y[400:525]\n",
    "y_cv[125:] = y[1400:1525]\n",
    "#1\n",
    "X_train = X_cv[50:]\n",
    "y_train = y_cv[50:]\n",
    "X_test = X_cv[:50]\n",
    "y_test = y_cv[:50]\n",
    "nb1 = NB()\n",
    "nb1.fit(X_train, y_train)\n",
    "score1 = nb1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "X_train = X_cv[:50] + X_cv[100:]\n",
    "y_train = np.zeros(200)\n",
    "y_train[:50] = y_cv[:50]\n",
    "y_train[50:] = y_cv[100:]\n",
    "X_test = X_cv[50:100]\n",
    "y_test = y_cv[50:100]\n",
    "nb2 = NB()\n",
    "nb2.fit(X_train, y_train)\n",
    "score2 = nb2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "X_train = X_cv[:100] + X_cv[150:]\n",
    "y_train = np.zeros(200)\n",
    "y_train[:100] = y_cv[:100]\n",
    "y_train[100:] = y_cv[150:]\n",
    "X_test = X_cv[100:150]\n",
    "y_test = y_cv[100:150]\n",
    "nb3 = NB()\n",
    "nb3.fit(X_train, y_train)\n",
    "score3 = nb3.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "X_train = X_cv[:150] + X_cv[120:]\n",
    "y_train = np.zeros(200)\n",
    "y_train[:150] = y_cv[:150]\n",
    "y_train[150:] = y_cv[200:]\n",
    "X_test = X_cv[150:200]\n",
    "y_test = y_cv[150:200]\n",
    "nb4 = NB()\n",
    "nb4.fit(X_train, y_train)\n",
    "score4 = nb4.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "X_train = X_cv[:200]\n",
    "y_train = y_cv[:200]\n",
    "X_test = X_cv[200:]\n",
    "y_test = y_cv[200:]\n",
    "nb5 = NB()\n",
    "nb5.fit(X_train, y_train)\n",
    "score5 = nb5.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of cross validation is 0.584\n"
     ]
    }
   ],
   "source": [
    "print \"The score of cross validation is \" + str((score1+score2+score3+score4+score5)/5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in order to ingore the words in english.stop\n",
    "filenames_stop = sorted(glob(op.join('..', 'data', 'english.stop')))\n",
    "texts_stop = open('english.stop').read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words2(texts):\n",
    "    vocabulary = Counter([])\n",
    "    for text in texts:\n",
    "        #pass\n",
    "        list1 = list(word for word in text.split() if word not in texts_stop)\n",
    "        dir1 = Counter(list1)\n",
    "        vocabulary = vocabulary + dir1\n",
    "    vocabulary = dict(vocabulary)\n",
    "    n_features = len(vocabulary)\n",
    "\n",
    "    counts = np.zeros(n_features)\n",
    "    j=0\n",
    "    for i in vocabulary:\n",
    "        counts[j] = vocabulary[i]\n",
    "        j +=1\n",
    "    \n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TrainMultinomialNB2(C, D1):  \n",
    "    y = zip(*D1)[1]\n",
    "    D = zip(*D1)[0]\n",
    "    V,counts = count_words2(D)\n",
    "    N = len(D)\n",
    "    prior = np.zeros(len(C))\n",
    "    condprob = np.zeros((len(V), len(C)))\n",
    "    i = 0\n",
    "    for c in C:\n",
    "        Nc = CountDocsInClass(y,c)\n",
    "        prior[i] = float(Nc)/N \n",
    "        text_c = ConcatenateTextOfAllDocsInClass(D1, c)\n",
    "        \n",
    "        V1 = text_c.split()\n",
    "        T_ct = np.zeros(len(V))\n",
    "        ii = 0\n",
    "        for m in V:\n",
    "            for l in V1:\n",
    "                if l == m:\n",
    "                    T_ct[ii] += 1\n",
    "            ii += 1\n",
    "\n",
    "        j = 0\n",
    "        for t in V:\n",
    "            condprob[j][i] = float(T_ct[j]+1)/(sum(T_ct+1))\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return V, prior, condprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NB2(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        D1 = zip(X, y)\n",
    "        self.C = [0, 1]\n",
    "        self.V, self.prior, self.condprob = TrainMultinomialNB2(self.C, D1)\n",
    "        return\n",
    "\n",
    "    def predict(self, X):\n",
    "        d = X\n",
    "        return ApplyMultinomialNB(self.C,self.V, self.prior, self.condprob, d)\n",
    "        #return (np.random.randn(len(X)) > 0).astype(np.int)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        correct = 0\n",
    "        for i in np.arange(0, len(X)):\n",
    "            if ApplyMultinomialNB(self.C, self.V, self.prior, self.condprob, X[i]) == y[i]:\n",
    "                correct += 1\n",
    "        return float(correct) / len(X)  \n",
    "        #return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = NB2()\n",
    "nb.fit(texts[::2], y[::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score after ingoring the “stop words” in the file is 0.832\n"
     ]
    }
   ],
   "source": [
    "print \"The score after ingoring the “stop words” in the file is \" + str(nb.score(texts[1::2], y[1::2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
